{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Section III - Feature Extraction and Linear Regression\n",
    "\n",
    "### Filter Methods\n",
    "\n",
    "#### Part A -  Feature Extraction\n",
    "The following feature extraction stragetices are common to performing data science. For this project, Principal Component Analysis was successful performed while Linear Discriminant Analysis and Kernal PCA required a more powerful CPU. Even though the project was run on a fairly power desktop with the GPU being utilized, these two feature extraction technique does not utilize the processing power of the GPU and therefore resulted in memory errors.   \n",
    "\n",
    "* **Principal Component Analysis** — we get accustomed with Zillow's dataset and do basic data analysis: we collect basic statistics, plot a correlation matrix, compare train and test distributions.\n",
    "\n",
    "* **Linear Discriminat Analysis** — we keep on getting memory errors when running a Linear Discriminant Analysis on the full dataset. The algortihm was reviously tested on a slice of the data which performed properly.\n",
    "\n",
    "* **Kernal PCA** — we keep on getting memory errors when running a PCA on the full dataset. The algortihm was reviously tested on a slice of the data which performed properly.\n",
    "\n",
    "#### Part B -  Multiple Linear Regression\n",
    "We use the multiple linear regression as our baseline model to compare the other regression models. Ideally, we would like to feed our entire list of features into our multiple regression model, but due to computational constraints we will only feed it the principal components we selected from earlier.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np   #Mathematics library\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import pandas as pd  #manage datasets\n",
    "import seaborn as sea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The FinishMissing.csv file is an output from the missing data appendix where every feature had a speicif strategy to deal with the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90811, 52)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy varies creaated, drop some features, Imputed missing data, \n",
    "df = pd.read_csv('FinishMissing.csv')\n",
    "df=df.drop('Unnamed: 0',axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upper 0.3373916901448827\n",
      "-0.31575874699922946\n",
      "0.16328760928602803\n",
      "0.01081647157282662\n"
     ]
    }
   ],
   "source": [
    "#Drop outliers before splitting ex and y\n",
    "avg = df['logerror'].mean()\n",
    "std = df['logerror'].std()\n",
    "upper_outlier = avg + 2*std\n",
    "lower_outlier = avg - 2*std\n",
    "print('upper', upper_outlier)\n",
    "print(lower_outlier)\n",
    "print(std)\n",
    "print(avg)\n",
    "#round up to drop outliers, til reasonable\n",
    "df=df[ df.logerror > -0.32 ]\n",
    "df=df[ df.logerror < 0.34 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88113, 52)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############Create Dummy variables for Categorical data\n",
    "df=pd.get_dummies(df,columns=['taxdelinquencyflag','fireplaceflag','propertyzoningdesc','propertycountylandusecode','hashottuborspa','airconditioningtypeid','architecturalstyletypeid','buildingqualitytypeid','buildingclasstypeid','decktypeid','fips','heatingorsystemtypeid','pooltypeid10','pooltypeid2','pooltypeid7','propertylandusetypeid','regionidcounty','regionidcity','regionidzip','regionidneighborhood','storytypeid','typeconstructiontypeid','month','day'],drop_first=True)\n",
    "\n",
    "#Change dataframe name to help check track of major changes to dataset\n",
    "dataset=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split into response and features\n",
    "X = dataset.iloc[:, 2:].values\n",
    "y = dataset.iloc[:,1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################### Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Feature Scaling required for Neural Network & Feature Extraction\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Feature Extraction,  PCA (n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### DIMENSIONALITY REDUCTION : PRINCIPAL COMPONENT ANALYSIS(PCA)\n",
    "# Applying PCA * requires feature scaling\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 10) # number of principal components explain variance, use '0' first\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "X = np.concatenate((X_train,X_test),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B:  Multiple Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0068916993432414503"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "# Predicting the Test set results\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00580249100635\n",
      "0.0761740835609\n"
     ]
    }
   ],
   "source": [
    "########### MEAN SQUARED ERROR\n",
    "\n",
    "# Mean Square Error Train/Test Splt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred, sample_weight=None, multioutput='uniform_average')\n",
    "print(mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00580486590399\n",
      "0.0761896705859\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross-validation with all three features\n",
    "###For Feature Extraction Run codes below before cross validation\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(regressor, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "mse_scores = -scores\n",
    "# calculate the average MSE\n",
    "print(mse_scores.mean())\n",
    "rmse_kfold = np.sqrt(mse_scores.mean())\n",
    "print(rmse_kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change dataframe name to help check track of major changes to dataset\n",
    "dataset=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split into response and features\n",
    "X = dataset.iloc[:, 2:].values\n",
    "y = dataset.iloc[:,1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################### Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################### Feature Scaling required for Neural Network & Feature Extraction\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Feature Extraction ,  PCA (n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### DIMENSIONALITY REDUCTION : PRINCIPAL COMPONENT ANALYSIS(PCA)\n",
    "# Applying PCA * requires feature scaling\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 20) # number of principal components explain variance, use '0' first\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "X = np.concatenate((X_train,X_test),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B:  Multiple Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0068758460307265555"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "# Predicting the Test set results\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0057969458862\n",
      "0.0761376771789\n"
     ]
    }
   ],
   "source": [
    "########### MEAN SQUARED ERROR\n",
    "\n",
    "# Mean Square Error Train/Test Splt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred, sample_weight=None, multioutput='uniform_average')\n",
    "print(mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00580459954181\n",
      "0.0761879225456\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross-validation with all three features\n",
    "###For Feature Extraction Run codes below before cross validation\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(regressor, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "mse_scores = -scores\n",
    "# calculate the average MSE\n",
    "print(mse_scores.mean())\n",
    "rmse_kfold = np.sqrt(mse_scores.mean())\n",
    "print(rmse_kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Change dataframe name to help check track of major changes to dataset\n",
    "dataset=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split into response and features\n",
    "X = dataset.iloc[:, 2:].values\n",
    "y = dataset.iloc[:,1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################### Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################### Feature Scaling required for Neural Network & Feature Extraction\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Feature Extraction, , PCA (n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### DIMENSIONALITY REDUCTION : PRINCIPAL COMPONENT ANALYSIS(PCA)\n",
    "# Applying PCA * requires feature scaling\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 10) # number of principal components explain variance, use '0' first\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "X = np.concatenate((X_train,X_test),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B:  Multiple Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0068839178529979941"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "# Predicting the Test set results\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00580110489136\n",
      "0.0761649846804\n"
     ]
    }
   ],
   "source": [
    "########### MEAN SQUARED ERROR\n",
    "\n",
    "# Mean Square Error Train/Test Splt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred, sample_weight=None, multioutput='uniform_average')\n",
    "print(mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00580466302437\n",
      "0.0761883391627\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross-validation with all three features\n",
    "###For Feature Extraction Run codes below before cross validation\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(regressor, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "mse_scores = -scores\n",
    "# calculate the average MSE\n",
    "print(mse_scores.mean())\n",
    "rmse_kfold = np.sqrt(mse_scores.mean())\n",
    "print(rmse_kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
