{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Section III - Feature Extraction and Regressions\n",
    "\n",
    "### Filter Methods\n",
    "\n",
    "#### Part A -  Feature Extraction\n",
    "The following feature extraction stragetices are common to performing data science. For this project, Principal Component Analysis was successful performed while Linear Discriminant Analysis and Kernal PCA required a more powerful CPU. Even though the project was run on a fairly power desktop with the GPU being utilized, these two feature extraction technique does not utilize the processing power of the GPU and therefore resulted in memory errors.   \n",
    "\n",
    "* **Principal Component Analysis** — we get accustomed with Zillow's dataset and do basic data analysis: we collect basic statistics, plot a correlation matrix, compare train and test distributions.\n",
    "\n",
    "* **Linear Discriminat Analysis** — we keep on getting memory errors when running a Linear Discriminant Analysis on the full dataset. The algortihm was reviously tested on a slice of the data which performed properly.\n",
    "\n",
    "* **Kernal PCA** — we keep on getting memory errors when running a PCA on the full dataset. The algortihm was reviously tested on a slice of the data which performed properly.\n",
    "\n",
    "#### Part B -  Multiple Linear Regression\n",
    "The initial data set from zillow includes a separate file for the transactions,which includes the log error of actual sales price versus Zillow estimates, and a training data set that has 52 features.  \n",
    "\n",
    "\n",
    "\n",
    "#### Part C - Support Vector Regression\n",
    "The goal of the inital exploratory analysis is to assess if there is some strange oddities with the data. We will also track to see if there any interesting trends that we could help us determine which features are important. We may also need to do certain data transformation in order to produce meanful analysis.\n",
    "\n",
    "#### Part D -  Decision Tree Regression\n",
    "The initial data set from zillow includes a separate file for the transactions,which includes the log error of actual sales price versus Zillow estimates, and a training data set that has 52 features.  \n",
    "\n",
    "#### Part E -  Random Forest Regressions\n",
    "The initial data set from zillow includes a separate file for the transactions,which includes the log error of actual sales price versus Zillow estimates, and a training data set that has 52 features.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np   #Mathematics library\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import pandas as pd  #manage datasets\n",
    "import seaborn as sea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The FinishMissing.csv file is an output from the missing data appendix where every feature had a speicif strategy to deal with the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90811, 52)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy varies creaated, drop some features, Imputed missing data, \n",
    "df = pd.read_csv('FinishMissing.csv')\n",
    "df=df.drop('Unnamed: 0',axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upper 0.3373916901448827\n",
      "-0.31575874699922946\n",
      "0.16328760928602803\n",
      "0.01081647157282662\n"
     ]
    }
   ],
   "source": [
    "#Drop outliers before splitting ex and y\n",
    "avg = df['logerror'].mean()\n",
    "std = df['logerror'].std()\n",
    "upper_outlier = avg + 2*std\n",
    "lower_outlier = avg - 2*std\n",
    "print('upper', upper_outlier)\n",
    "print(lower_outlier)\n",
    "print(std)\n",
    "print(avg)\n",
    "#round up to drop outliers, til reasonable\n",
    "df=df[ df.logerror > -0.32 ]\n",
    "df=df[ df.logerror < 0.34 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88113, 52)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############Create Dummy variables for Categorical data\n",
    "df=pd.get_dummies(df,columns=['taxdelinquencyflag','fireplaceflag','propertyzoningdesc','propertycountylandusecode','hashottuborspa','airconditioningtypeid','architecturalstyletypeid','buildingqualitytypeid','buildingclasstypeid','decktypeid','fips','heatingorsystemtypeid','pooltypeid10','pooltypeid2','pooltypeid7','propertylandusetypeid','regionidcounty','regionidcity','regionidzip','regionidneighborhood','storytypeid','typeconstructiontypeid','month','day'],drop_first=True)\n",
    "\n",
    "#Change dataframe name to help check track of major changes to dataset\n",
    "dataset=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split into response and features\n",
    "X = dataset.iloc[:, 2:].values\n",
    "y = dataset.iloc[:,1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################### Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################### Feature Scaling required for Neural Network & Feature Extraction\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### DIMENSIONALITY REDUCTION : PRINCIPAL COMPONENT ANALYSIS(PCA)\n",
    "# Applying PCA * requires feature scaling\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 10) # number of principal components explain variance, use '0' first\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "X = np.concatenate((X_train,X_test),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REGRESSION I: Fitting Multiple Linear Regression to the Training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0068865756569827536"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "# Predicting the Test set results\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00580368263483\n",
      "0.0761819049042\n"
     ]
    }
   ],
   "source": [
    "########### MEAN SQUARED ERROR\n",
    "\n",
    "# Mean Square Error Train/Test Splt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred, sample_weight=None, multioutput='uniform_average')\n",
    "print(mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00580487541193\n",
      "0.0761897329824\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross-validation with all three features\n",
    "###For Feature Extraction Run codes below before cross validation\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(regressor, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "mse_scores = -scores\n",
    "# calculate the average MSE\n",
    "print(mse_scores.mean())\n",
    "rmse_kfold = np.sqrt(mse_scores.mean())\n",
    "print(rmse_kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REGRESSION II Fitting Support Vector Regression(SVR) to the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ceribrum\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "C:\\Users\\ceribrum\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############from sklearn.svm import SVR\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "sc_y = StandardScaler()\n",
    "y = sc_y.fit_transform(y)\n",
    "\n",
    "regressor = SVR(kernel = 'rbf')\n",
    "regressor.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting a new result\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_pred = sc_y.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00600909524724\n",
      "0.0775183542604\n"
     ]
    }
   ],
   "source": [
    "# Mean Square Error Train/Test Splt\n",
    "mse = mean_squared_error(y_test, y_pred, sample_weight=None, multioutput='uniform_average')\n",
    "print(mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.01051584688\n",
      "1.00524417277\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross-validation with all three features\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(regressor, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "mse_scores = -scores\n",
    "# calculate the average MSE\n",
    "print(mse_scores.mean())\n",
    "rmse_kfold = np.sqrt(mse_scores.mean())\n",
    "print(rmse_kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REGRESSION III: Fitting Decision Tree Regression to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(random_state = 0)\n",
    "regressor.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.35043223377\n",
      "1.16208099277\n"
     ]
    }
   ],
   "source": [
    "# Predicting a new result\n",
    "y_pred = regressor.predict(X_test)\n",
    "# Mean Square Error\n",
    "mse = mean_squared_error(y_test, y_pred, sample_weight=None, multioutput='uniform_average')\n",
    "print(mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1722338982\n",
      "1.47385002568\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross-validation with all three features\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(regressor, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "mse_scores = -scores\n",
    "# calculate the average MSE\n",
    "print(mse_scores.mean())\n",
    "rmse_kfold = np.sqrt(mse_scores.mean())\n",
    "print(rmse_kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REGRESSION IV:Fitting Random Forest Regression to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=0,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regressor = RandomForestRegressor(n_estimators = 10, random_state = 0)\n",
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.144197100071\n",
      "0.379732932561\n"
     ]
    }
   ],
   "source": [
    "# Predicting a new result\n",
    "y_pred = regressor.predict(X_test)\n",
    "# Mean Square Error\n",
    "mse = mean_squared_error(y_test, y_pred, sample_weight=None, multioutput='uniform_average')\n",
    "print(mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16609565103\n",
      "1.07985908851\n"
     ]
    }
   ],
   "source": [
    "# 10-fold cross-validation with all three features\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(regressor, X, y, cv=10, scoring='neg_mean_squared_error')\n",
    "mse_scores = -scores\n",
    "# calculate the average MSE\n",
    "print(mse_scores.mean())\n",
    "rmse_kfold = np.sqrt(mse_scores.mean())\n",
    "print(rmse_kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
